{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "from utils import cross_val_splits, kicktipp_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIALS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.read_csv(\"matches.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticEstimator(BaseEstimator):\n",
    "    def __init__(self, result) -> None:\n",
    "        super().__init__()\n",
    "        self.result = result\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = np.zeros((len(X), 2))\n",
    "        results[:] = self.result\n",
    "        return results\n",
    "\n",
    "class RandomEstimator(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        df_result_counts = df_matches[[\"host_goals\", \"guest_goals\"]].value_counts().reset_index()\n",
    "        df_result_counts[\"p\"] = df_result_counts[\"count\"] / len(df_matches)\n",
    "        self.df_result_counts = df_result_counts\n",
    "\n",
    "    def predict(self, X):\n",
    "        df_result_counts = self.df_result_counts\n",
    "        result_indices = np.random.choice(df_result_counts.index, len(X), p=df_result_counts[\"p\"].values)\n",
    "        return df_result_counts.loc[result_indices, [\"host_goals\", \"guest_goals\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_simple_models(scoring):\n",
    "    splits = cross_val_splits(df_matches)\n",
    "    labels = df_matches[[\"host_goals\", \"guest_goals\"]].values\n",
    "    static_results = [[0, 0], [1, 1], [1, 0], [2, 1], [0, 1]]\n",
    "    static_result_scores = []\n",
    "    for result in static_results:\n",
    "        scores = cross_val_score(StaticEstimator(result), df_matches, labels, cv=splits, scoring=scoring)\n",
    "        static_result_scores.append(scores)\n",
    "    static_result_scores = np.array(static_result_scores)\n",
    "\n",
    "    random_scores = cross_val_score(RandomEstimator(), df_matches, labels, cv=splits, scoring=scoring)\n",
    "\n",
    "    all_scores = np.vstack((static_result_scores, random_scores))\n",
    "    labels = [f\"{res[0]}:{res[1]}\" for res in static_results] + [\"random\"]\n",
    "    for score, label in zip(all_scores, labels):\n",
    "        print(f\"{label}: {score.mean():.2f} +- {score.std():.2f}\")\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "    split_test_seasons = [df_matches.iloc[split[1][0]][\"season\"] for  split in splits]\n",
    "    axs[0].plot(all_scores.T, label=labels)\n",
    "    axs[0].set_xticks(range(len(split_test_seasons)), split_test_seasons, rotation=90, ha='center')\n",
    "    real_results = [340, 335, 328, 325, 320, 313]\n",
    "    axs[0].scatter([len(split_test_seasons)-1] * len(real_results), real_results, s=8, marker=\"x\", color=\"black\")\n",
    "    axs[0].set_xlabel(\"Test season\")\n",
    "    axs[0].set_ylabel(\"Score\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title(\"Cross-validation scores\")\n",
    "\n",
    "    axs[1].boxplot(all_scores.T)\n",
    "    axs[1].set_xlabel(\"Strategy\")\n",
    "    axs[1].set_xticks(range(1, len(labels) +1), labels, rotation=45, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "evaluate_simple_models(scoring=kicktipp_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches with team context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"matches_with_context.csv\", index_col=\"id\")\n",
    "df_full = df_full.fillna(0)\n",
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df_full.drop(columns=[\"host_name\", \"guest_name\", \"host_goals\", \"guest_goals\"])\n",
    "y_all = df_full[[\"host_goals\", \"guest_goals\"]].values\n",
    "print(X_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundingEstimator(BaseEstimator):\n",
    "    def __init__(self, regressor, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.regressor = regressor\n",
    "        self.regressor.set_params(**kwargs)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.regressor.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        raw = self.regressor.predict(X)\n",
    "        return np.round(raw)\n",
    "    \n",
    "    def get_params(self, deep = False):\n",
    "        params = self.regressor.get_params(deep)\n",
    "        params[\"regressor\"] = self.regressor\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.regressor.set_params(**params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = cross_val_splits(X_all, start=3)\n",
    "def objective(trial):\n",
    "    rf_criterion = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"])\n",
    "    rf_max_depth = trial.suggest_int('max_depth', 2, 15)\n",
    "    rf_n_estimators = trial.suggest_int('n_estimators', 50, 50)\n",
    "    rf_min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n",
    "    estimator = RoundingEstimator(RandomForestRegressor(\n",
    "        n_estimators=rf_n_estimators, \n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "rf_study = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"rf\", load_if_exists=True)\n",
    "rf_study.optimize(objective, n_trials=TRIALS//4)\n",
    "print(rf_study.best_value, rf_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    algo = trial.suggest_categorical(\"algo\", [\"xgb\", \"lgbm\"])\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 15)\n",
    "    reg_lambda=trial.suggest_float(\"lambda\", 0.0, 2.0)\n",
    "    reg_alpha=trial.suggest_float(\"alpha\", 0.0, 2.0)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0, 0.5)\n",
    "    num_leaves=trial.suggest_int(\"num_leaves\", 2, 31)\n",
    "    min_child_weight = trial.suggest_float(\"min_child_weight\", 0, 15)\n",
    "    if algo == \"xgb\":\n",
    "        estimator = RoundingEstimator(XGBRegressor(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-xgb\", [\"reg:squarederror\", \"reg:squaredlogerror\", \"reg:absoluteerror\"]),\n",
    "        ))\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = RoundingEstimator(MultiOutputRegressor(LGBMRegressor(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-lgbm\", [\"rmse\", \"mae\", \"poisson\", \"mape\"]),\n",
    "        )))\n",
    "\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "gb_study = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"gb\", load_if_exists=True)\n",
    "gb_study.optimize(objective, n_trials=TRIALS)\n",
    "print(gb_study.best_value, gb_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gb_model(study, rounder=RoundingEstimator):\n",
    "    params = study.best_params\n",
    "    algo = params[\"algo\"]\n",
    "    n_estimators = params[\"n_estimators\"]\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    reg_lambda=params[\"lambda\"]\n",
    "    reg_alpha=params[\"alpha\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    num_leaves=params[\"num_leaves\"]\n",
    "    min_child_weight = params[\"min_child_weight\"]\n",
    "    if algo == \"xgb\":\n",
    "        estimator = rounder(XGBRegressor(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-xgb\"],\n",
    "        ))\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = rounder(MultiOutputRegressor(LGBMRegressor(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-lgbm\"],\n",
    "        )))\n",
    "    return estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"baseline\": StaticEstimator([2, 1]),\n",
    "    \"rf\": RoundingEstimator(RandomForestRegressor(\n",
    "        criterion=rf_study.best_params['criterion'], \n",
    "        max_depth=rf_study.best_params['max_depth'], \n",
    "        n_estimators=rf_study.best_params['n_estimators'], \n",
    "        min_samples_split=rf_study.best_params['min_samples_split']\n",
    "    )),\n",
    "    \"gb\": build_gb_model(gb_study)\n",
    "}\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for model in models.values():\n",
    "    print(f\"Scoring {model}\")\n",
    "    scores = cross_val_score(model, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "labels = models.keys()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "split_test_seasons = [df_matches.iloc[split[1][0]][\"season\"] for  split in splits]\n",
    "axs[0].plot(all_scores.T, label=labels)\n",
    "axs[0].set_xticks(range(len(split_test_seasons)), split_test_seasons, rotation=90, ha='center')\n",
    "real_results = [340, 335, 328, 325, 320, 313]\n",
    "axs[0].scatter([len(split_test_seasons)-1] * len(real_results), real_results, s=8, marker=\"x\", color=\"black\")\n",
    "axs[0].set_xlabel(\"Test season\")\n",
    "axs[0].set_ylabel(\"Score\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Cross-validation scores\")\n",
    "\n",
    "axs[1].boxplot(all_scores.T)\n",
    "axs[1].set_xlabel(\"Strategy\")\n",
    "axs[1].set_xticks(range(1, len(labels) +1), labels, rotation=90, ha=\"center\")\n",
    "fig.tight_layout()\n",
    "!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_gb_model(gb_study)\n",
    "train_idx, test_idx = splits[-2]\n",
    "X_train, X_test = X_all.iloc[train_idx], X_all.iloc[test_idx]\n",
    "y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_raw = model.regressor.predict(X_train)\n",
    "\n",
    "df_full_train = df_full.iloc[train_idx].copy()\n",
    "df_full_train[\"team1Goals_pred\"] = y_train_pred[:, 0]\n",
    "df_full_train[\"team2Goals_pred\"] = y_train_pred[:, 1]\n",
    "df_full_train[\"team1Goals_pred_raw\"] = y_train_pred_raw[:, 0]\n",
    "df_full_train[\"team2Goals_pred_raw\"] = y_train_pred_raw[:, 1]\n",
    "df_full_train[[\"season\", \"match_day\", \"host_name\", \"guest_name\", \"host_goals\", \"guest_goals\", \"team1Goals_pred\", \"team2Goals_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train[\"team2Goals_pred_raw\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train[[\"team1Goals_pred\", \"team2Goals_pred\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierEstimator(BaseEstimator):\n",
    "    def __init__(self, classifier, max_goals=3, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.classifier.set_params(**kwargs)\n",
    "        self.target_encoder = LabelEncoder()\n",
    "        self.max_goals = max_goals\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_results(results: np.ndarray, max_goals=3):\n",
    "        results = results.copy()\n",
    "        match_goals = results.sum(axis=1)\n",
    "        for i, res in enumerate(results):\n",
    "            while match_goals[i] > max_goals:\n",
    "                results[i, 0] = np.max((0, results[i, 0] - 1))\n",
    "                results[i, 1] = np.max((0, results[i, 1] - 1))\n",
    "                match_goals[i] = results[i, 0] + results[i, 1]\n",
    "        return results.astype(results.dtype)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = ClassifierEstimator.clip_results(y, self.max_goals)\n",
    "        results = [f\"{a[0]}:{a[1]}\" for a in y]\n",
    "        self.target_encoder.fit(results)\n",
    "        results_encoded = self.target_encoder.transform(results)\n",
    "        self.classifier.fit(X, results_encoded)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        raw = self.classifier.predict(X)\n",
    "        results = self.target_encoder.inverse_transform(raw)\n",
    "        y = [[int(a.split(\":\")[0]), int(a.split(\":\")[1])] for a in results]\n",
    "        return np.array(y)\n",
    "    \n",
    "    def get_params(self, deep = False):\n",
    "        params = self.classifier.get_params(deep)\n",
    "        params[\"classifier\"] = self.classifier\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.classifier.set_params(**params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    rf_criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"log_loss\", \"entropy\"])\n",
    "    rf_max_depth = trial.suggest_int('max_depth', 2, 15)\n",
    "    rf_n_estimators = trial.suggest_int('n_estimators', 50, 50)\n",
    "    rf_min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n",
    "    max_goals = trial.suggest_int('max_goals', 1, 5)\n",
    "    estimator = ClassifierEstimator(RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators, \n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        n_jobs=-1\n",
    "    ), max_goals=max_goals)\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "rf_study = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"rf-classifier\", load_if_exists=True)\n",
    "rf_study.optimize(objective, n_trials=TRIALS//4)\n",
    "print(rf_study.best_value, rf_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective(trial):\n",
    "    algo = trial.suggest_categorical(\"algo\", [\"xgb\", \"lgbm\"])\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 15)\n",
    "    reg_lambda=trial.suggest_float(\"lambda\", 0.0, 2.0)\n",
    "    reg_alpha=trial.suggest_float(\"alpha\", 0.0, 2.0)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0, 0.5)\n",
    "    num_leaves=trial.suggest_int(\"num_leaves\", 2, 31)\n",
    "    min_child_weight = trial.suggest_float(\"min_child_weight\", 0, 15)\n",
    "    max_goals = trial.suggest_int('max_goals', 1, 5)\n",
    "    if algo == \"xgb\":\n",
    "        estimator = ClassifierEstimator(XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-xgb\", [\"multi:softmax\"]),\n",
    "        ), max_goals=max_goals)\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = ClassifierEstimator(LGBMClassifier(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-lgbm\", [\"multiclass\"]),\n",
    "        ), max_goals=max_goals)\n",
    "\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "gb_study_clf = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"gb-classifier\", load_if_exists=True)\n",
    "gb_study_clf.optimize(objective, n_trials=TRIALS//2)\n",
    "print(gb_study_clf.best_value, gb_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gb_model_classifier(study):\n",
    "    params = study.best_params\n",
    "    algo = params[\"algo\"]\n",
    "    n_estimators = params[\"n_estimators\"]\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    reg_lambda=params[\"lambda\"]\n",
    "    reg_alpha=params[\"alpha\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    num_leaves=params[\"num_leaves\"]\n",
    "    min_child_weight = params[\"min_child_weight\"]\n",
    "    max_goals = params[\"max_goals\"]\n",
    "    if algo == \"xgb\":\n",
    "        estimator = ClassifierEstimator(XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-xgb\"],\n",
    "        ), max_goals=max_goals)\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = ClassifierEstimator(LGBMClassifier(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-lgbm\"],\n",
    "        ), max_goals=max_goals)\n",
    "    return estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"baseline\": StaticEstimator([2, 1]),\n",
    "    \"rf\": ClassifierEstimator(RandomForestClassifier(\n",
    "        criterion=rf_study.best_params['criterion'], \n",
    "        max_depth=rf_study.best_params['max_depth'], \n",
    "        n_estimators=rf_study.best_params['n_estimators'], \n",
    "        min_samples_split=rf_study.best_params['min_samples_split']\n",
    "    ), max_goals=rf_study.best_params[\"max_goals\"]),\n",
    "    \"gb\": build_gb_model_classifier(gb_study_clf)\n",
    "}\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for model in models.values():\n",
    "    print(f\"Scoring {model}\")\n",
    "    scores = cross_val_score(model, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "labels = models.keys()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "split_test_seasons = [df_matches.iloc[split[1][0]][\"season\"] for  split in splits]\n",
    "axs[0].plot(all_scores.T, label=labels)\n",
    "axs[0].set_xticks(range(len(split_test_seasons)), split_test_seasons, rotation=90, ha='center')\n",
    "axs[0].scatter([len(split_test_seasons)-1] * len(real_results), real_results, s=8, marker=\"x\", color=\"black\")\n",
    "axs[0].set_xlabel(\"Test season\")\n",
    "axs[0].set_ylabel(\"Score\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Cross-validation scores\")\n",
    "\n",
    "axs[1].boxplot(all_scores.T)\n",
    "axs[1].set_xlabel(\"Strategy\")\n",
    "axs[1].set_xticks(range(1, len(labels) +1), labels, rotation=90, ha=\"center\")\n",
    "fig.tight_layout()\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot_importance( final_model.regressor.estimators_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceEstimator(BaseEstimator):\n",
    "    def __init__(self, regressor, base=0, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.regressor = regressor\n",
    "        self.regressor.set_params(**kwargs)\n",
    "        self.base = base\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_diff = y[:, 0] - y[:, 1]\n",
    "        self.min_diff = np.min(y_diff)\n",
    "        self.regressor.fit(X, y_diff - self.min_diff)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_diff = self.regressor.predict(X) + self.min_diff\n",
    "        results = np.zeros((len(X), 2))\n",
    "        results[:, 1] = self.base\n",
    "        results[:, 0] = results[:, 1] + y_diff\n",
    "        return np.round(np.clip(results, 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    rf_criterion = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"])\n",
    "    rf_max_depth = trial.suggest_int('max_depth', 2, 15)\n",
    "    rf_n_estimators = trial.suggest_int('n_estimators', 50, 50)\n",
    "    rf_min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n",
    "    base = trial.suggest_float(\"base\", 0, 2)\n",
    "    estimator = DifferenceEstimator(RandomForestRegressor(\n",
    "        n_estimators=rf_n_estimators, \n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        n_jobs=-1,\n",
    "    ), base=base)\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "rf_study_diff = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"rf-diff\", load_if_exists=True)\n",
    "rf_study_diff.optimize(objective, n_trials=TRIALS//4)\n",
    "print(rf_study_diff.best_value, rf_study_diff.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    algo = trial.suggest_categorical(\"algo\", [\"xgb\", \"lgbm\"])\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 15)\n",
    "    reg_lambda=trial.suggest_float(\"lambda\", 0.0, 2.0)\n",
    "    reg_alpha=trial.suggest_float(\"alpha\", 0.0, 2.0)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0, 0.5)\n",
    "    num_leaves=trial.suggest_int(\"num_leaves\", 2, 31)\n",
    "    min_child_weight = trial.suggest_float(\"min_child_weight\", 0, 15)\n",
    "    base = trial.suggest_float(\"base\", 0, 2)\n",
    "    if algo == \"xgb\":\n",
    "        estimator = DifferenceEstimator(XGBRegressor(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-xgb\", [\"reg:squarederror\", \"reg:squaredlogerror\", \"reg:absoluteerror\"]),\n",
    "        ), base=base)\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = DifferenceEstimator(LGBMRegressor(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=trial.suggest_categorical(\"objective-lgbm\", [\"rmse\", \"mae\", \"poisson\", \"mape\"]),\n",
    "        ), base=base)\n",
    "\n",
    "    scores = cross_val_score(estimator, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "gb_study_diff = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=\"gb-diff\", load_if_exists=True)\n",
    "gb_study_diff.optimize(objective, n_trials=TRIALS*4)\n",
    "print(gb_study_diff.best_value, gb_study_diff.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gb_model_diff(study):\n",
    "    params = study.best_params\n",
    "    algo = params[\"algo\"]\n",
    "    n_estimators = params[\"n_estimators\"]\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    reg_lambda=params[\"lambda\"]\n",
    "    reg_alpha=params[\"alpha\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    num_leaves=params[\"num_leaves\"]\n",
    "    min_child_weight = params[\"min_child_weight\"]\n",
    "    base = params[\"base\"]\n",
    "    if algo == \"xgb\":\n",
    "        estimator = DifferenceEstimator(XGBRegressor(\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            max_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-xgb\"],\n",
    "        ), base=base)\n",
    "    elif algo == \"lgbm\":\n",
    "        estimator = DifferenceEstimator(LGBMRegressor(\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            n_estimators = n_estimators,\n",
    "            max_depth = max_depth,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_weight=min_child_weight,\n",
    "            objective=params[\"objective-lgbm\"],\n",
    "        ), base=base)\n",
    "    return estimator\n",
    "    \n",
    "\n",
    "model = build_gb_model_diff(gb_study_diff)\n",
    "train_idx, test_idx = splits[-2]\n",
    "X_train, X_test = X_all.iloc[train_idx], X_all.iloc[test_idx]\n",
    "y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_raw = model.regressor.predict(X_train) + model.min_diff\n",
    "\n",
    "df_full_train = df_full.iloc[train_idx].copy()\n",
    "df_full_train[\"team1Goals_pred\"] = y_train_pred[:, 0]\n",
    "df_full_train[\"team2Goals_pred\"] = y_train_pred[:, 1]\n",
    "df_full_train[\"diff_pred\"] = y_train_pred_raw\n",
    "df_full_train[[\"season\", \"match_day\", \"host_name\", \"guest_name\", \"host_goals\", \"guest_goals\", \"diff_pred\", \"team1Goals_pred\", \"team2Goals_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train[[\"team1Goals_pred\", \"team2Goals_pred\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train[[\"diff_pred\"]].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_values = np.linspace(0, 2, 20)\n",
    "scores = np.zeros_like(base_values)\n",
    "\n",
    "for i, base_value in enumerate(base_values):\n",
    "    model.base = base_value\n",
    "    s = cross_val_score(model, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    scores[i] = np.mean(s)\n",
    "\n",
    "plt.plot(base_values, scores)\n",
    "print(np.max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"baseline\": StaticEstimator([2, 1]),\n",
    "    \"rf\": DifferenceEstimator(RandomForestRegressor(\n",
    "        criterion=rf_study_diff.best_params['criterion'], \n",
    "        max_depth=rf_study_diff.best_params['max_depth'], \n",
    "        n_estimators=rf_study_diff.best_params['n_estimators'], \n",
    "        min_samples_split=rf_study_diff.best_params['min_samples_split'],\n",
    "    ), base=rf_study_diff.best_params[\"base\"]),\n",
    "    \"gb\": build_gb_model_diff(gb_study_diff)\n",
    "}\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for model in models.values():\n",
    "    print(f\"Scoring {model}\")\n",
    "    scores = cross_val_score(model, X_all, y_all, cv=splits, verbose=1, scoring=kicktipp_scoring)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "labels = models.keys()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "split_test_seasons = [df_matches.iloc[split[1][0]][\"season\"] for  split in splits]\n",
    "axs[0].plot(all_scores.T, label=labels)\n",
    "axs[0].set_xticks(range(len(split_test_seasons)), split_test_seasons, rotation=90, ha='center')\n",
    "real_results = [340, 335, 328, 325, 320, 313]\n",
    "axs[0].scatter([len(split_test_seasons)-1] * len(real_results), real_results, s=8, marker=\"x\", color=\"black\")\n",
    "axs[0].set_xlabel(\"Test season\")\n",
    "axs[0].set_ylabel(\"Score\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Cross-validation scores\")\n",
    "\n",
    "axs[1].boxplot(all_scores.T)\n",
    "axs[1].set_xlabel(\"Strategy\")\n",
    "axs[1].set_xticks(range(1, len(labels) +1), labels, rotation=90, ha=\"center\")\n",
    "fig.tight_layout()\n",
    "!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
